{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heirarchical Cluster probability models\n",
    "\n",
    "The question is given a heirarchical clustering model what is the actual probability distribution on inputs?\n",
    "\n",
    "Here I want to explore how to calculate the actual input probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n",
      "/home/tbjackso/NeoNeo\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, MultivariateNormal, Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.utils.clip_grad import clip_grad_value_, clip_grad_norm_\n",
    "\n",
    "import string\n",
    "from random import *\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "import subprocess, os, glob\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:2'\n",
    "print (device)\n",
    "\n",
    "print (os.getcwd())\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "from dennytools.silence import *\n",
    "from Modules.conv import Convolutor\n",
    "from Distributions import MultinouliClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EMMB(nn.Module):\n",
    "    def __init__(self, D, M, eta=0.95, FF=None):\n",
    "        super().__init__()\n",
    "        self.register_buffer('D', torch.tensor(D))\n",
    "        self.register_buffer('M', torch.tensor(M))\n",
    "        \n",
    "        self.pi = nn.Parameter(torch.ones((1,M))/M, requires_grad=False)\n",
    "        self.mu = nn.Parameter(torch.rand((1,M,D)), requires_grad=False)\n",
    "        \n",
    "        self.FF = nn.Softmax(dim=-1) if FF is None else FF\n",
    "        \n",
    "    def responsibility(self, x, raw=False, verbose=False):\n",
    "        with silence(not verbose):\n",
    "            print ('x', torch.isnan(x).sum()>0)\n",
    "            print ('pi', torch.isnan(self.pi).sum()>0)\n",
    "            print ('mu', torch.isnan(self.mu).sum()>0)\n",
    "            # Negative Log Liklihood of the multinoulis\n",
    "            logpi = torch.log(self.pi+1e-18)\n",
    "            print ('logpi',torch.isnan(logpi).sum()>0)\n",
    "            logmu = torch.log(self.mu+1e-18)\n",
    "            print ('logmu',torch.isnan(logmu).sum()>0)\n",
    "            logommu = torch.log(1-self.mu+1e-18)\n",
    "            print ('logommu',torch.isnan(logommu).sum()>0)\n",
    "            mix_log_probs = (logpi + (x*logmu + (1-x)*logommu).sum(-1))\n",
    "            print ('mix_log_probs', torch.isnan(mix_log_probs).sum()>0)\n",
    "            resp = F.softmax(mix_log_probs, dim=-1)\n",
    "            print ('resp', torch.isnan(resp).sum()>0)\n",
    "\n",
    "        if (raw):\n",
    "            return mix_log_probs\n",
    "        return resp\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1,self.D)\n",
    "        y = self.responsibility(x, raw=True)\n",
    "        return self.FF(y)\n",
    "        \n",
    "    def learn(self, x):\n",
    "        x = x.view(-1,1,self.D)\n",
    "        \n",
    "        # This learns via EM algorithm\n",
    "        # E step: calculates the responsibility of each mode for each point\n",
    "        # M step: calculates the pis and mus\n",
    "        rik = self.responsibility(x)\n",
    "        if (torch.isnan(rik).sum()>0):\n",
    "            self.responsibility(x, verbose=True)\n",
    "            raise ValueError(\"NaN Found\")\n",
    "        rk = rik.sum(0, keepdim=True)\n",
    "        # rik:  (BS, M) -> (BS, M, 1)\n",
    "        newmu = (rik.unsqueeze(-1)*x).sum(0, keepdim=True)/(rk.unsqueeze(-1)+1e-18)\n",
    "        newpi = rk/x.shape[0]\n",
    "        \n",
    "        # The M update\n",
    "        self.pi.data = eta*self.pi.data + (1-eta)*newpi\n",
    "        self.mu.data = eta*self.mu.data + (1-eta)*newmu\n",
    "        \n",
    "    def params(self, sort=True):\n",
    "        pi = self.pi.squeeze()\n",
    "        mu = self.mu.squeeze()\n",
    "        if (sort):\n",
    "            pi, pin = pi.sort(descending=True)\n",
    "            mu = mu[pin]\n",
    "        return (pi, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "BS = 100\n",
    "\n",
    "C = 2\n",
    "M = 1\n",
    "D = 4\n",
    "\n",
    "MMDataset = MultinouliClassification(C, M, D, noise=0.1, samples=5000)\n",
    "# Training dataset\n",
    "train = torch.utils.data.DataLoader(MMDataset, batch_size=BS, shuffle=True)\n",
    "\n",
    "print (len(train))\n",
    "print (len(MMDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, label) = next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0.9000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.9000, 0.9000, 0.9000, 0.9000]])\n",
      "tensor([[0.9986, 0.0014],\n",
      "        [0.0014, 0.9986],\n",
      "        [0.9000, 0.1000],\n",
      "        [0.0014, 0.9986],\n",
      "        [0.0014, 0.9986]])\n",
      "torch.Size([100, 2])\n",
      "tensor([-1.1146, -7.7063])\n",
      "tensor([1., 0., 0., 0.])\n",
      "tensor([0.9000, 0.9000, 0.9000, 0.9000])\n",
      "tensor(-1.1146)\n",
      "tensor(-7.7063)\n",
      "tensor(-1.1132)\n"
     ]
    }
   ],
   "source": [
    "print (data[:5])\n",
    "print (MMDataset.archetypes)\n",
    "\n",
    "archetypes = MMDataset.archetypes\n",
    "pi = torch.ones((1,C*M))/C*M\n",
    "\n",
    "# For fun... lets see the likelihood that a given data sample belongs to a given \n",
    "# archetype (Ideally these archetypes will be learned by the clustering method)\n",
    "\n",
    "def responsibility(x, pi, mu, raw=False, verbose=False):\n",
    "    with silence(not verbose):\n",
    "        x = x.unsqueeze(1)\n",
    "        print ('x', torch.isnan(x).sum()>0)\n",
    "        print ('pi', torch.isnan(pi).sum()>0)\n",
    "        print ('mu', torch.isnan(mu).sum()>0)\n",
    "        # Negative Log Liklihood of the multinoulis\n",
    "        logpi = torch.log(pi+1e-18)\n",
    "        print ('logpi',torch.isnan(logpi).sum()>0)\n",
    "        logmu = torch.log(mu+1e-18)\n",
    "        print ('logmu',torch.isnan(logmu).sum()>0)\n",
    "        logommu = torch.log(1-mu+1e-18)\n",
    "        print ('logommu',torch.isnan(logommu).sum()>0)\n",
    "        mix_log_probs = (logpi + (x*logmu + (1-x)*logommu).sum(-1))\n",
    "        print ('mix_log_probs', torch.isnan(mix_log_probs).sum()>0)\n",
    "        resp = F.softmax(mix_log_probs, dim=-1)\n",
    "        print ('resp', torch.isnan(resp).sum()>0)\n",
    "    \n",
    "    if (raw):\n",
    "        return mix_log_probs\n",
    "    return resp\n",
    "\n",
    "resp = responsibility(data, pi, archetypes)\n",
    "print (resp[:5])\n",
    "\n",
    "MLP = responsibility(data, pi, archetypes, raw=True)\n",
    "print (MLP.shape)\n",
    "print (MLP[0])\n",
    "print (data[0])\n",
    "print (archetypes[-1])\n",
    "TP = 0\n",
    "for i in range(C):\n",
    "    p = pi[0,i]\n",
    "    for j in range(D):\n",
    "        p *= (data[0,j]*archetypes[i,j] + (1-data[0,j])*(1-archetypes[i,j]))\n",
    "    print (np.log(p))\n",
    "    TP += p\n",
    "    \n",
    "print (np.log(TP))\n",
    "# Checks out lets do this thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.9**9 * 0.1\n",
    "print (p)\n",
    "print (np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "tensor([0.5139, 0.4861])\n",
      "tensor([[0.9037, 0.9094, 0.9121, 0.9037],\n",
      "        [0.9127, 0.1121, 0.0939, 0.1037]])\n",
      "tensor([[0.9000, 0.1000, 0.1000, 0.1000],\n",
      "        [0.9000, 0.9000, 0.9000, 0.9000]])\n",
      "tensor([0.5139, 0.4861])\n"
     ]
    }
   ],
   "source": [
    "# Lets make a model and do that training thing\n",
    "\n",
    "eta = 0.95\n",
    "epochs = 5\n",
    "\n",
    "model = EMMB(D, C*M, eta=eta)\n",
    "# model.mu.data = archetypes.unsqueeze(0)\n",
    "\n",
    "for e in range(epochs):\n",
    "    print (\"Epoch: {}\".format(e))\n",
    "    for (data, label) in train:\n",
    "        model.learn(data)\n",
    "        \n",
    "(pi, mu) = model.params()\n",
    "\n",
    "print (pi)\n",
    "mask = (mu > 1e-30).float()\n",
    "print (mu*mask)\n",
    "print (archetypes)\n",
    "print (pi)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5139, 0.4861])\n",
      "tensor([[0.9037, 0.9094, 0.9121, 0.9037],\n",
      "        [0.9127, 0.1121, 0.0939, 0.1037]])\n",
      "torch.Size([2, 4])\n",
      "tensor([0.5139, 0.4861])\n",
      "tensor([[0.0043, 0.9957],\n",
      "        [0.9948, 0.0052]])\n",
      "\n",
      "-----------\n",
      "\n",
      "tensor([0.9037, 0.9094, 0.9121, 0.9037])\n",
      "tensor([0.9000, 0.9000, 0.9000, 0.9000])\n",
      "\n",
      "-------------\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "-------------\n",
      "\n",
      "tensor([[4., 1.],\n",
      "        [1., 4.]])\n",
      "tensor([[6., 9.],\n",
      "        [9., 6.]])\n",
      "tensor([[0.6561, 0.9000],\n",
      "        [0.9000, 0.6561]])\n",
      "tensor([[0.6561, 0.9000],\n",
      "        [0.9000, 0.6561]])\n",
      "tensor([0.7781, 0.7781])\n",
      "tensor([-0.2510, -0.2510])\n"
     ]
    }
   ],
   "source": [
    "(pi,mu) = model.params()\n",
    "print (pi)\n",
    "print (mu)\n",
    "print (mu.shape)\n",
    "archetypes = archetypes\n",
    "resp = responsibility(archetypes, pi.unsqueeze(0), mu.unsqueeze(0))\n",
    "mask = (resp>1e-10).float()\n",
    "print (pi)\n",
    "print (resp*mask)\n",
    "print (\"\\n-----------\\n\")\n",
    "print (mu[0])\n",
    "print (archetypes[1])\n",
    "print (\"\\n-------------\")\n",
    "data = (archetypes>0.5).float()\n",
    "print (data)\n",
    "print (\"-------------\\n\")\n",
    "\n",
    "# raise ValueError('STOP')\n",
    "\n",
    "ones = data @ data.transpose(0,1)\n",
    "zeros = (1-data) @ (1-data).transpose(0,1)\n",
    "overlap = ones + zeros\n",
    "dist = 10 - overlap\n",
    "print(overlap)\n",
    "print (dist)\n",
    "\n",
    "print (0.9**overlap)\n",
    "P = (0.9**overlap + 0.1**dist)\n",
    "print (P)\n",
    "print (P.mean(-1))\n",
    "print (torch.log(P.mean(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (archetypes>0.5).float()\n",
    "print (archetypes.shape)\n",
    "print (responsibility(data, torch.ones(1,C)/C, archetypes.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000]])\n",
      "tensor([[[0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.1000, 0.1000],\n",
      "         [0.9000, 0.1000, 0.9000,  ..., 0.1000, 0.9000, 0.9000],\n",
      "         [0.1000, 0.9000, 0.1000,  ..., 0.9000, 0.9000, 0.1000],\n",
      "         ...,\n",
      "         [0.1000, 0.1000, 0.9000,  ..., 0.9000, 0.1000, 0.9000],\n",
      "         [0.1000, 0.1000, 0.1000,  ..., 0.1000, 0.9000, 0.9000],\n",
      "         [0.1000, 0.1000, 0.1000,  ..., 0.9000, 0.9000, 0.1000]]])\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000]])\n",
      "tensor([[[0.1020, 0.1060, 0.0962,  ..., 0.1040, 0.1076, 0.1060],\n",
      "         [0.9046, 0.1026, 0.9008,  ..., 0.1022, 0.9024, 0.8968],\n",
      "         [0.0970, 0.9018, 0.1058,  ..., 0.8938, 0.9032, 0.1042],\n",
      "         ...,\n",
      "         [0.1046, 0.0976, 0.9084,  ..., 0.9020, 0.0922, 0.9040],\n",
      "         [0.1018, 0.1014, 0.1052,  ..., 0.1118, 0.9048, 0.9072],\n",
      "         [0.1006, 0.0900, 0.1024,  ..., 0.9006, 0.8996, 0.1038]]])\n"
     ]
    }
   ],
   "source": [
    "# This cell demonstrates how to do offline EM clustering\n",
    "# It it kept solely for historical reference\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# mu = torch.rand((1,C*M,D))\n",
    "mu = archetypes.unsqueeze(0)\n",
    "pi = torch.ones((1,C*M))/(C*M)\n",
    "\n",
    "print (pi)\n",
    "print (mu)\n",
    "\n",
    "# E step: calculates the responsibility of each mode for each point\n",
    "# M step: calculates the pis and mus\n",
    "\n",
    "# Since we are doing this with batches we need to accumulate the statistics\n",
    "for e in range(epochs):\n",
    "    print ('Epoch: {}'.format(e))\n",
    "    newmu = torch.zeros_like(mu)\n",
    "    rk = torch.zeros_like(pi)\n",
    "    N = 0\n",
    "    for (data, label) in train:\n",
    "        data = data.view(-1,D)\n",
    "        N += data.shape[0]\n",
    "        rik = responsibility(data, pi, mu)\n",
    "        if (torch.isnan(rik).sum()>0):\n",
    "            responsibility(data, pi, mu, verbose=True)\n",
    "            raise ValueError(\"NaN Found\")\n",
    "        rk += rik.sum(0, keepdim=True)\n",
    "        # rik:  (BS, M) -> (BS, M, 1)\n",
    "        # data: (BS, D) -> (BS, 1, D)\n",
    "        newmu += (rik.unsqueeze(-1)*data.unsqueeze(1)).sum(0, keepdim=True)\n",
    "        \n",
    "    pi = rk/N\n",
    "    mu = newmu/rk.unsqueeze(-1)\n",
    "\n",
    "print (pi)\n",
    "print (mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "binary = []\n",
    "for i in range(16):\n",
    "    binary.append(np.unpackbits(np.array([i], dtype=np.uint8)))\n",
    "binary = np.array(binary)\n",
    "print (binary)\n",
    "\n",
    "D = 4\n",
    "P = np.zeros([2]*D)\n",
    "print (P.shape)\n",
    "\n",
    "P[0,0,0,0] = 1\n",
    "P[1,1,1,1] = 1\n",
    "P[0,0,0,1] = 0\n",
    "P[1,1,1,0] = 0\n",
    "\n",
    "P = P/P.sum()\n",
    "print (P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Helper function\n",
    "def str2bool(v):\n",
    "    return 0 if v.lower() in (\"yes\", \"true\", \"t\", \"1\") else 1\n",
    "\n",
    "class factor():\n",
    "    def __init__(self, array, variables):\n",
    "        # None factor - The empty Factor allows me to avoid being concerned\n",
    "        #about edge cases user side\n",
    "        if (len(variables) == 0):\n",
    "            self.data = np.array([])\n",
    "            self.vars = []\n",
    "            self.isEmpty = True\n",
    "            return\n",
    "        \n",
    "        # Construct the data from an array passed in\n",
    "        data = np.array(array)\n",
    "        # Check that the correct number of dimension labels is passed in\n",
    "        if (len(variables) != data.ndim):\n",
    "            raise ValueError(\"Number of labels does not match number of dimensions\")\n",
    "        \n",
    "        self.data = data\n",
    "        self.vars = variables\n",
    "        self.isEmpty = False\n",
    "\n",
    "    # Implementation of the factor restrict function\n",
    "    def restrict(self, var, value):\n",
    "        # If empty factor return an new empty factor\n",
    "        if self.isEmpty:\n",
    "            return factor([],[])\n",
    "        \n",
    "        # This essentially helps translate human readable values to internal numbers\n",
    "        # note that internally a 0 value is the T value for that dimension which \n",
    "        # is a little counter intuitive hence bothering with a conversion\n",
    "        if isinstance(value, str):\n",
    "            value = str2bool(value)\n",
    "        \n",
    "        # Find the dimension of interest\n",
    "        index = self.vars.index(var)\n",
    "        # The numpy .take function takes a slice where dim \"index\" is value \"value\".\n",
    "        ndata = self.data.take(value, index).squeeze()\n",
    "        nvars = self.vars.copy()\n",
    "        nvars.remove(var) # Delete the variable from the list\n",
    "        return factor(ndata, nvars)\n",
    "\n",
    "    # Implements the sumout operation of a function over a variable\n",
    "    def sumout(self, var):\n",
    "        # Handle empty factor\n",
    "        if self.isEmpty:\n",
    "            return factor([],[])\n",
    "        \n",
    "        # Get dimension number corresponding to variable\n",
    "        index = self.vars.index(var)\n",
    "        # the numpy sum operation sums together the restricted slices for every \n",
    "        # value of the passed axis\n",
    "        ndata = self.data.sum(index)\n",
    "        nvars = self.vars.copy()\n",
    "        nvars.remove(var) # remove the summed out variable from the list\n",
    "        \n",
    "        return factor(ndata, nvars)\n",
    "\n",
    "    def normalize(self):\n",
    "        if self.isEmpty:\n",
    "            return\n",
    "        # Does this need explaining?\n",
    "        self.data = self.data/self.data.sum()\n",
    "        \n",
    "    # This is a non-critical function it just re-arranges the dimensions\n",
    "    # so the variables appear in a different order. I implemented it simply \n",
    "    # because I once wanted to compare resultsand didn't like manually re-ordering them\n",
    "    def reorder(self, order):\n",
    "        if self.isEmpty:\n",
    "            return\n",
    "        \n",
    "        for i, v in enumerate(order):\n",
    "            curInd = self.vars.index(v)\n",
    "            self.data = np.swapaxes(self.data, i, curInd) \n",
    "            self.vars[curInd] = self.vars[i]\n",
    "            self.vars[i] = v\n",
    "    \n",
    "    # Here I overwrite the multiplication operator so I can do f1*f2 :)\n",
    "    def __mul__(self, other):\n",
    "        # Handle cases where one factor or more factors are empty\n",
    "        if (self.isEmpty and other.isEmpty):\n",
    "            return factor([],[])\n",
    "        elif (self.isEmpty):\n",
    "            return factor(other.data, other.vars)\n",
    "        elif (other.isEmpty):\n",
    "            return factor(self.data, self.vars)\n",
    "        # Multiplication of factors means broadcasting,\n",
    "        # but we need to be the correct shape first\n",
    "        \n",
    "        # Step 1: Align the two factors inputing dummy dimensions when required\n",
    "            # We need to start by finding the matching components \n",
    "            # and reordering indicesSo they line up\n",
    "        ovars = other.vars.copy()\n",
    "        odata = other.data.copy()\n",
    "        \n",
    "        svars = self.vars.copy()\n",
    "        sdata = self.data.copy()\n",
    "        \n",
    "        # This loop finds when a variable in other is in self,\n",
    "        # If it finds a match it will move swap the variable order so they match\n",
    "        # if self does not have the variable in other, a dummy dimension is added\n",
    "        #  and the variable is added to a copy of self's list\n",
    "        for i in range(len(ovars)):\n",
    "            ovar = ovars[i]\n",
    "            # other and self share a variable\n",
    "            if (ovar in svars):\n",
    "                curInd = svars.index(ovar)\n",
    "                # reorder the variables in self so the the shared variables are in the same dimension\n",
    "                sdata = np.swapaxes(sdata, i, curInd) \n",
    "                svars[curInd] = svars[i]\n",
    "                svars[i] = ovar\n",
    "            else:\n",
    "                # Insert a placeholder dimension in self\n",
    "                sdata = np.expand_dims(sdata, i)\n",
    "                svars.insert(i, ovar)\n",
    "        \n",
    "        # Finish by adding necessary number of placeholders to the end of other\n",
    "        for i in range(len(ovars), len(svars)):\n",
    "            odata = np.expand_dims(odata, i)\n",
    "        \n",
    "        # now just multiply the numpy arrays. Broadcasting will ensure matching \n",
    "        # dimensions ar multiplied together and placeholder dimensions will be \n",
    "        # multiplied by all options\n",
    "        newData = (sdata*odata).squeeze()\n",
    "        \n",
    "        return factor(newData, svars) # return a new factor\n",
    "    \n",
    "    # Helper\n",
    "    def copy(self):\n",
    "        return factor(self.data.copy(), self.vars.copy())\n",
    "\n",
    "    # This makes print (f) a little nicer though most times I use f.table() below\n",
    "    def __str__(self):\n",
    "        return \"Factor with variables: {} \\n and Data:\\n {}\".format(self.vars, self.data)\n",
    "    \n",
    "    # Pretty print the truth table of self\n",
    "    def table(self, p=2):\n",
    "        if (self.isEmpty):\n",
    "            return\n",
    "        \n",
    "        N = len(self.vars)\n",
    "        pstr = \"\"\n",
    "        for i in range(N):\n",
    "            pstr += \"| {} \"\n",
    "        dstr = pstr + \"| {:.{p}f} \"\n",
    "        pstr += \"| {} \"\n",
    "        \n",
    "        temp = pstr.format(*(self.vars + [\"P\"]))\n",
    "        print (temp)\n",
    "        print (\"{:->{width}}\".format(\"\",width=len(temp)))\n",
    "        \n",
    "        for i in range(2**N):\n",
    "            b = tuple(map(int, \"{:0>{width}b}\".format(i, width=N)))\n",
    "            d = list(map(lambda x: \"T\" if x == 0 else \"F\", b)) + [self.data[b]]\n",
    "            print (dstr.format(*d, p=p))\n",
    "        print (\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TBJ",
   "language": "python",
   "name": "tbj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
