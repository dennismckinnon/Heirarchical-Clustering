{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Clustering\n",
    "\n",
    "This has been inspired by a variety of bits. Most notably [1](https://arxiv.org/pdf/1606.04695.pdf) which used attention mechanisms and \"replanning points\" to learn sequences of actions. Most key to the inspiration this gave is that temporal convolution could be used. Also the increment operator and attention mechanisms could be useful.\n",
    "\n",
    "\n",
    "I do need to learn about transformer networks. Its possible there is a relationship there.\n",
    "\n",
    "There are a couple key points to this (elaborate)\n",
    "\n",
    "- if the window is used as a convolution over time this is essentially a interesting ngram finder    \n",
    "- the key component of EM algorithm is that those modes that are activated become updated.    \n",
    "- we can make this deep with feedback to correct a variety of errors    \n",
    "- attention mechanism of which pieces of the mode should be activated next could be used with some fuzziness. lets say the mean was \"test\" if we have seen \"te\" then the attention mechanism would weight the importance of the \"s\" higher. if a \"t\" was found next it might still improve activation but less so then if the \"s\" was found but more then if the \"a\" was found.    \n",
    "\n",
    "\n",
    "***\n",
    "1) [Vezhnevets, A., Mnih, V., Osindero, S., Graves, A., Vinyals, O., & Agapiou, J. (2016). Strategic attentive writer for learning macro-actions. In Advances in neural information processing systems (pp. 3486-3494).](https://arxiv.org/pdf/1606.04695.pdf)    \n",
    "2) [Gutenburg Project](http://www.gutenberg.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n",
      "/home/tbjackso/NeoNeo\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Normal, MultivariateNormal, Categorical\n",
    "\n",
    "import string\n",
    "from random import *\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "import subprocess, os, glob\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:5'\n",
    "print (device)\n",
    "\n",
    "print (os.getcwd())\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "# from multimodal import MultiModalClassification\n",
    "# from GMM import *\n",
    "from dennytools.silence import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "I want to work with some true sequence data so I'm going to work with text. In particular stories from the Project Gutenburg [2](http://www.gutenberg.org) in this case Alice in wonderland. I actually want to experiment with a few traditional RNNs too so lets get to work. the first goal is that we have to create a dataset out of the text which i have saved as Alice.txt. We must strip the header and footer. Everything else can stay i think. But then we have to convert it into onehot encoded by letters (i'm not doing word to vec here) and then the dataset must return snippets of text from various locations of a specified length. How am I going to do that? I don't really know.... The easiest would be to load it as a 1D vector of numbers I think and then one hot encode and finally trim reshape and/or use a convolution to create window snippets. Note we should probably pad with empty space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'‘': \"'\", '’': \"'\", '“': \"'\", '”': \"'\"}\n",
      "144530\n",
      "3356\n",
      "\u0000\u0001\u0002\u0003\u0004\u0005\u0006\t\n",
      "\u000e\u000f\u0010\u0011\u0012\u0013\u0014\u0015\u0016\u0017\u0018\u0019\u001a\u001b\u001c",
      "\u001d",
      "\u001e",
      "\u001f !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
      "70\n",
      "['\\n', ' ', '!', \"'\", '(', ')', '*', ',', '-', '.', '0', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "++++++++++++++++++++++++\n",
      "[]\n",
      "[]\n",
      "------------------------\n",
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b",
      "\f",
      "\n",
      "100\n",
      "hello World! []()\n",
      "[17, 14, 21, 21, 24, 94, 58, 24, 27, 21, 13, 62, 94, 84, 86, 69, 70]\n",
      "hello World! []()\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "header = True\n",
    "headerstr = '^\\*\\*\\* START OF THIS PROJECT GUTENBERG'\n",
    "footerstr = '^\\*\\*\\* END OF THIS PROJECT GUTENBERG'\n",
    "\n",
    "def replace(string, substitutions):\n",
    "\n",
    "    substrings = sorted(substitutions, key=len, reverse=True)\n",
    "    regex = re.compile('|'.join(map(re.escape, substrings)))\n",
    "    return regex.sub(lambda match: substitutions[match.group(0)], string)\n",
    "\n",
    "preprocess = {chr(8216):\"'\", chr(8217):\"'\", chr(8220):\"'\", chr(8221):\"'\"}\n",
    "print (preprocess)\n",
    "\n",
    "linecount = 0\n",
    "for line in open('Alice.txt'):\n",
    "#     print (\"-----------\")\n",
    "#     print (line)\n",
    "#     print (line.encode().decode('ascii', 'replace'))\n",
    "    \n",
    "    if (header):\n",
    "        # check if end of header is found and switch header off\n",
    "        if (re.search(headerstr, line) is not None):\n",
    "            header = False\n",
    "    else:\n",
    "        # check if start of footer is found and stop if so otherwise add line to text\n",
    "        if (re.search(footerstr, line) is not None):\n",
    "            break\n",
    "        else:\n",
    "            linecount += 1\n",
    "            \n",
    "            text += replace(line, preprocess)#.decode('ascii')#.replace('\\n', ' ')\n",
    "print (len(text))\n",
    "print (linecount)\n",
    "# print (text)\n",
    "\n",
    "# Now lets convert into something useful\n",
    "full = [chr(i) for i in range(128)]\n",
    "print(''.join(full))\n",
    "unique = sorted(list(set(text)))\n",
    "print (len(unique))\n",
    "print (unique)\n",
    "\n",
    "print (\"++++++++++++++++++++++++\")\n",
    "print ([u for u in unique if u not in string.printable])\n",
    "print ([ord(u) for u in unique if u not in full])\n",
    "print (\"------------------------\")\n",
    "\n",
    "import string\n",
    "print (string.printable)\n",
    "print (len(string.printable))\n",
    "alpha = string.printable\n",
    "l2n = {a:i for (i,a) in enumerate(alpha)}\n",
    "n2l = alpha\n",
    "\n",
    "\n",
    "\n",
    "tststr = \"hello World! []()\"\n",
    "tstnum = list(map(lambda a:l2n[a], tststr))\n",
    "print (tststr)\n",
    "print (tstnum)\n",
    "bckstr = ''.join(list(map(lambda n: alpha[n], tstnum)))\n",
    "print (bckstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "{'a': 0, 'b': 1, 'c': 2}\n",
      "4\n",
      "0\n",
      "unknown letter: d\n",
      "100\n",
      "3\n",
      "unknown letter: d\n",
      "100\n",
      "unknown letter: e\n",
      "101\n",
      "unknown letter: f\n",
      "102\n",
      "unknown letter: g\n",
      "103\n",
      "[0, 1, 2, 3, 3, 3, 3]\n",
      "unknown letter: d\n",
      "100\n",
      "unknown letter: e\n",
      "101\n",
      "unknown letter: f\n",
      "102\n",
      "unknown letter: g\n",
      "103\n",
      "torch.Size([1, 10, 4])\n",
      "unknown letter: d\n",
      "100\n",
      "unknown letter: e\n",
      "101\n",
      "unknown letter: f\n",
      "102\n",
      "unknown letter: g\n",
      "103\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "class ConvLN():\n",
    "    def __init__(self, alpha=string.printable):\n",
    "        self.alpha = alpha\n",
    "        self.dict = {a:i for (i,a) in enumerate(alpha)}\n",
    "        self.size = len(alpha) + 1\n",
    "    \n",
    "    def l2n(self, a):\n",
    "        try:\n",
    "            n = self.dict[a]\n",
    "        except KeyError:\n",
    "            print (\"unknown letter: {}\".format(a))\n",
    "            print (ord(a))\n",
    "            n = len(self.alpha)\n",
    "        return n\n",
    "    \n",
    "    def s2n(self, s):\n",
    "        return [self.l2n(a) for a in s]\n",
    "    \n",
    "    def s2t(self, s, l):\n",
    "        ls = len(s)\n",
    "        r = (l-(ls%l))\n",
    "        d = ls + r\n",
    "        \n",
    "        nums = torch.tensor(self.s2n(s), dtype=torch.long).unsqueeze(1)\n",
    "        ten = torch.zeros((ls, self.size))\n",
    "        out = torch.zeros((d, self.size))\n",
    "        ten = ten.scatter_(1, nums, 1)\n",
    "        out[:ls] = ten\n",
    "        out[ls:] = ten[:r]\n",
    "        out = out.view(-1, l, self.size)\n",
    "        return out\n",
    "            \n",
    "conv = ConvLN('abc')\n",
    "print (conv.alpha)\n",
    "print (conv.dict)\n",
    "print (conv.size)\n",
    "print (conv.l2n('a'))\n",
    "print (conv.l2n('d'))\n",
    "print (conv.s2n('abcdefg'))\n",
    "\n",
    "print (conv.s2t('abcdefg', 10).shape)\n",
    "print (conv.s2t('abcdefg', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 36, 47, 44, 38, 40, 68, 54, 94, 36, 39, 57, 40, 49, 55, 56, 53, 40, 54, 94, 44, 49, 94, 58, 50, 49, 39, 40, 53, 47, 36, 49, 39, 96, 96, 47, 14, 32, 18, 28, 94, 38, 10, 27, 27, 24, 21, 21, 96, 96, 55, 43, 40, 94, 48, 44, 47, 47, 40, 49, 49, 44, 56, 48, 94, 41, 56, 47, 38, 53, 56, 48, 94, 40, 39, 44, 55, 44, 50, 49, 94, 3, 75, 0, 96, 96, 96, 96, 96, 38, 43]\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]])\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "conv = ConvLN()\n",
    "\n",
    "data = conv.s2n(text)\n",
    "print (data[:100])\n",
    "\n",
    "data = conv.s2t(text, len(text)).long()\n",
    "print (data[0,10:15])\n",
    "print (conv.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now onto some more interesting things\n",
    "\n",
    "To learn sequence data we can learn a probability model p(x1, x2,..., xn) and then make it generative by conditioning, i.e p(x1,...,xn)/p(x1,...,xn-1). But this conditioning is potentially impractical.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TBJ",
   "language": "python",
   "name": "tbj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
